# -*- coding: utf-8 -*-
"""ML HW2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Da57fqr1l7Q1RBYLyXpjIdPn3TVhT2gb
"""

import numpy  as np 
import pandas as pd
import sys
import matplotlib.pyplot as plt
import scipy.io
import time


"""Q1.1"""

# loading dataset
# assuming fish are class positive
dataset1 = scipy.io.loadmat('q1_dataset.mat')

"""Stochastic Gradient Descent"""

def getCost(features, labels, weights):
    j = 0.0
    for x,y in zip(features, labels):
        y_pred = np.dot(weights, x.T)
        j += (y_pred - y) ** 2
    
    cost = j / (len(features) * 2.0)
    
    return cost

def stochastic_graddesc(feature_set, labels_set, weights, lr = 0.01):
    j_store = np.zeros(len(labels_set))

    index = 0
    for x,y in zip(feature_set, labels_set):
      j_store[index] = getCost(feature_set, labels_set, weights)
        
      print("--------------------------")
      print("iteration: ", index)
      print( "cost: ",j_store[index])
        
      y_pred = np.dot(weights, x.T)
      weights += lr * (x.T * (y - y_pred))/feature_set.shape[1]
        
      index += 1
            
    return weights, j_store

def displayTestResult(tp,tn,fp,fn):
  tp += 1
  tn += 1
  fp += 1
  fn += 1
  print("accuracy: ", (tp + tn) / (tp + tn + fp + fn))
  precision = tp / (tp+fp)
  recall = tp / (tp + fn)
  print("precision: ", precision)
  print("recall: ", recall)
  print("NPV: ", tn / (tn + fn))
  print("FPR: ", fp / (fp + tn))
  print("FDR: ", fp / (fp + tp))
  print("F1 score: ", 2*tp / (2*tp + fp + fn))
  print("F2 score: ",  (5 * precision * recall) / (4 * precision + recall))
  print("***Confusion Matrix***")
  print("TP: ",tp)
  print("TN: ",tn)
  print("FP: ",fp)
  print("FN: ",fn)

#making predictions
def test(test_set):
  results = np.zeros(test_set.shape[0])
  hits = 0
  sum = 0
  misses = 0
  tp = 0
  tn = 0
  fp = 0
  fn = 0

  for i in range(test_set.shape[0]):
    pred = 0
    for j in range(test_set.shape[1]):
      pred += theta[j]*test_set[i][j]
    print(pred)
    if abs(1-pred) > abs(pred):#(1-pred)**2 > (pred)**2:
      results[i] = 0;
    else:
      results[i] = 1;
 
  for i in range(test_set.shape[0]):
    if results[i] == dataset1["superclass_labels_test"][i][0]:
      hits += 1
      if results[i] == 1:
        tp += 1
      else:
        tn += 1
    else:
      misses +=1
      if results[i] == 1:
        fp += 1
      else:
        fn += 1
        
  displayTestResult(tp,tn,fp,fn)

#making predictions
def test_minibatch(test_set):
  results = np.zeros(test_set.shape[0])
  hits = 0
  sum = 0
  misses = 0
  tp = 0
  tn = 0
  fp = 0
  fn = 0

  for i in range(test_set.shape[0]):
    pred = 0
    for j in range(test_set.shape[1]):
      pred += theta[j]*test_set[i][j]
    print(np.mean(pred))
    if abs(1-np.mean(pred)) > abs(np.mean(pred)):#(1-pred)**2 > (pred)**2:
      results[i] = 0;
    else:
      results[i] = 1;
 
  for i in range(test_set.shape[0]):
    if results[i] == dataset1["superclass_labels_test"][i][0]:
      hits += 1
      if results[i] == 1:
        tp += 1
      else:
        tn += 1
    else:
      misses +=1
      if results[i] == 1:
        fp += 1
      else:
        fn += 1
        
  displayTestResult(tp,tn,fp,fn)

#train with inception set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["inception_features_train"].shape[1])
theta, cost = stochastic_graddesc( dataset1["inception_features_train"], dataset1["superclass_labels_train"],
                                                    t_init, 0.01)
stop = time.time()

print("TIME TO train stochastic gradient descent with inception in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF INCEPTION TRAINING SET-----")
test(dataset1["inception_features_test"])
stop = time.time()

print("TIME TO test stochastic gradient descent with inception in seconds: ", stop-start)

#train with HOG set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["hog_features_train"].shape[1])
theta, cost = stochastic_graddesc( dataset1["hog_features_train"], dataset1["superclass_labels_train"], t_init, 0.01)
stop = time.time()

print("TIME TO train stochastic gradient descent with HOG in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF HOG TRAINING SET-----")
test(dataset1["hog_features_test"])
stop = time.time()

print("TIME TO test stochastic gradient descent with HOG in seconds: ", stop-start)

"""Mini-Batch Gradient Descent"""

def minibatch_graddesc(x, y, weights, lr = 0.01 ,itr=1000,bs =25):
 
  j_store = np.zeros(itr)
  for j in range(itr):
    cost =0.0
    batch_i = np.random.permutation(y.shape[1])
    x = x[batch_i]
    y = y[batch_i]
    for i in range(0,y.shape[1],bs):
      bi_x = x[i:i+bs]
      bi_y = y[i:i+bs]

      c = getCost(bi_x, bi_y, weights);
      y_hat = np.dot(weights, bi_x.T)
      grad = bi_x.T * (bi_y - y_hat)
      weights = weights + lr * grad/x.shape[1]

    j_store[j] = np.mean(c)
    print("--------------------------")
    print("iteration: ", j)
    print("cost: ", j_store[j])
    
  return weights, j_store

#train with inception set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["inception_features_train"].shape[1])
theta, cost = minibatch_graddesc( dataset1["inception_features_train"], dataset1["superclass_labels_train"], t_init, 0.01 ,1000, 25)
stop = time.time()
print("TIME TO train minibatch gradient descent with inception in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF INCEPTION TRAINING SET-----")
test_minibatch(dataset1["inception_features_test"])
stop = time.time()

print("TIME TO test mini batch gradient descent with inception in seconds: ", stop-start)

#train with HOG set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["hog_features_train"].shape[1])
theta, cost = minibatch_graddesc( dataset1["hog_features_train"], dataset1["superclass_labels_train"], t_init, 0.01, 1000)
stop = time.time()

print("TIME TO train mini batch gradient descent with HOG in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF HOG TRAINING SET-----")
test_minibatch(dataset1["hog_features_test"])
stop = time.time()

print("TIME TO test mini batch gradient descent with HOG in seconds: ", stop-start)

"""Q 1.2

Full Batch Gradient Descent
"""

def fullbatch_graddesc(feature_set, label_set, weights, lr = 0.01, iterations = 1000):
  j_store = np.zeros(iterations)
    
  for i in range(iterations):
    j_store[i] = getCost(feature_set, label_set, weights)

    print("--------------------------")
    print("iteration: ", i)
    print( "cost: ",j_store[i])

    for x,y in zip(feature_set, label_set):
      y_pred = np.dot(weights, x.T)
      weights += lr * (x.T * (y - y_pred))/feature_set.shape[1]
    if i % 100 == 0:
      print("weights in iteration ", i, ": ", weights)
    
  return weights, j_store

#train with inception set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["inception_features_train"].shape[1])
theta, cost = fullbatch_graddesc( dataset1["inception_features_train"], dataset1["superclass_labels_train"], 
                                         t_init, 0.01 ,1000)
stop = time.time()
print("TIME TO train fullbatch gradient descent with inception in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF INCEPTION TRAINING SET-----")
test(dataset1["inception_features_test"])
stop = time.time()

print("TIME TO test fullbatch gradient descent with inception in seconds: ", stop-start)

#train with HOG set
start = time.time()
t_init = np.random.normal(0, 0.01, dataset1["hog_features_train"].shape[1])
theta, cost = fullbatch_graddesc( dataset1["hog_features_train"], dataset1["superclass_labels_train"], t_init, 0.01, 1000)
stop = time.time()

print("TIME TO train fullbatch gradient descent with HOG in seconds: ", stop-start)

start = time.time()
print("-----RESULTS OF HOG TRAINING SET-----")
test(dataset1["hog_features_test"])
stop = time.time()

print("TIME TO test fullbatch gradient descent with HOG in seconds: ", stop-start)

"""Q 1.3"""

import random
from sklearn.svm import SVC

def stratified_k_fold(features, labels, k):
  f = np.split( features, k )
  l = np.split( labels, k )

  return f, l

"""Q 1.4"""

# soft margin SVM training algo
def softMarginSVM( features, labels, c_vals, k): 
  accuracy_store = np.zeros((len(np.atleast_1d(c_vals)), k))

  for i in range(len(np.atleast_1d(c_vals))):
    print(i)
    for j in range(k):
      features_t = np.zeros((features[0].shape[0], features[0].shape[1]))
      labels_t = np.zeros((labels[0].shape[0], labels[0].shape[1]))

      features_v = np.zeros((features[0].shape[0], features[0].shape[1]))
      labels_v = np.zeros((labels[0].shape[0], labels[0].shape[1]))
      
      #prep data
      for m in range(k):
        if j != m:
          features_t = np.vstack((features_t, features[m]))
          labels_t = np.vstack((labels_t, labels[m]))
        else:
          features_v = np.vstack((features_v, features[m]))
          labels_v = np.vstack((labels_v, labels[m]))

      #SVM
      svm = SVC(kernel='linear', C=c_vals[i])
      svm.fit(features_t, labels_t)
      svm_acc = svm.score(features_v, labels_v)
      accuracy_store[i, j] = svm_acc
      print(svm_acc)
  return accuracy_store

def displayTestResult_SVM(tp,tn,fp,fn,macroMicroFlag = 0):
  print("accuracy: ", (tp + tn) / (tp + tn + fp + fn))
  tp += 1 
  tn += 1
  fn += 1
  fp += 1
  
  precision = tp / (tp+fp)
  recall = tp / (tp + fn)
  
  print("precision: ", precision)
  print("recall: ", recall)
  print("***Confusion Matrix***")
  print("TP: ",tp)
  print("TN: ",tn)
  print("FP: ",fp)
  print("FN: ",fn)
  if (macroMicroFlag == 1):
    #display macro averaging
    print("***Macro Averaging***")
    print("Macro Precision: ",precision)
    print("Macro Recall: ",recall)

    print("Macro NPV: ",(tn / (fn+tn)))
    print("Macro FPR: ",(fp / (fp+tn)))
    print("Macro FDR: ",(fp / (fp+tp)))
    print("Macro F1: ",((2* precision * recall) / (precision + recall)))
    print("Macro F2: ",((5* precision * recall) / (4*precision + recall)))

    #display micro averaging
    print("***Micro Averaging***")

    print("Micro Precision: ",precision)
    print("Micro Recall: ",recall)
    print("Micro NPV: ",(tn / (fn+tn)))
    print("Micro FPR: ",(fp / (fp+tn)))
    print("Micro FDR: ",(fp / (fp+tp)))
    print("Micro F1: ",((2* precision * recall) / (precision + recall)))
    print("Micro F2: ",((5* precision * recall) / (4*precision + recall)))

# train soft margin SVM using Inception features
start = time.time()
c_vals = np.array([0.01, 0.1, 1, 10, 100])
features, labels = stratified_k_fold( dataset1["inception_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies = softMarginSVM(features, labels, c_vals, 5)
stop = time.time()
print("TIME OF training soft margin SVM using INCEPTION features: ", stop-start)

# soft margin SVM test algo
def softMarginSVMTest(features_t, labels_t, features_test, labels_test, c_optimal):
    svm = SVC(kernel='linear', C=c_optimal)
    svm.fit(features_t, labels_t)
    pred = svm.predict(features_test).reshape(-1,1)
    test_SVM(pred,labels_test)

def test_SVM(prediction_set, test_label_set, avgFlag = 0):
  '''
  tp  = np.sum( np.logical_and( test_label_set, prediction_set) )
  fp = np.sum( np.logical_and( np.logical_not(test_label_set), prediction_set) )
  fn  = np.sum( np.logical_and( test_label_set, np.logical_not(prediction_set)) )
  tn = np.sum( np.logical_and( np.logical_not(test_label_set), np.logical_not(prediction_set)) )
  '''
  results = np.zeros(prediction_set.shape[0])
  hits = 0
  sum = 0
  misses = 0
  tp = 0
  tn = 0
  fp = 0
  fn = 0
  for i in range(len(test_label_set)):
    if prediction_set[i] == dataset1["superclass_labels_test"][i][0]:
      hits += 1
      if prediction_set[i][0] == 1:
        tp += 1
      else:
        tn += 1
    else:
      misses +=1
      if prediction_set[i][0] == 1:
        fp += 1
      else:
        fn += 1 
  displayTestResult_SVM(tp,tn,fp,fn,avgFlag)

# obtain optimal value of C
start = time.time()
print(c_vals[np.argmax(accuracies)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies)]
c_val = np.array([optimum_c])

# test inception features on soft margin SVM 
print("***Testing Inception features on Soft Margin SVM***")
softMarginSVMTest(dataset1["inception_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["inception_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_c)
stop = time.time()
print("TIME OF testing soft margin SVM using INCEPTION features: ", stop-start)

# train soft margin SVM using HOG features
start = time.time()
c_vals = np.array([0.01, 0.1, 1, 10, 100])
features, labels = stratified_k_fold( dataset1["hog_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies = softMarginSVM(features, labels, c_vals, 5)

stop = time.time()
print("TIME OF training soft margin SVM using HOG features: ", stop-start)

start = time.time()
# obtain optimal value of C
print(c_vals[np.argmax(accuracies)%5], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies)%5]
c_val = np.array([optimum_c])

# test HOG features on soft margin SVM 
print("***Testing HOG features on Soft Margin SVM***")
softMarginSVMTest(dataset1["hog_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["hog_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_c)
stop = time.time()
print("TIME OF testing soft margin SVM using HOG features: ", stop-start)

"""Q 1.5"""

# hard margin SVM training algo
def hardMarginSVM( features, labels, g_vals, k): 
  accuracy_store = np.zeros((len(np.atleast_1d(g_vals)), k))

  for i in range(len(np.atleast_1d(g_vals))):
    print(i)
    for j in range(k):
      features_t = np.zeros((features[0].shape[0], features[0].shape[1]))
      labels_t = np.zeros((labels[0].shape[0], labels[0].shape[1]))

      features_v = np.zeros((features[0].shape[0], features[0].shape[1]))
      labels_v = np.zeros((labels[0].shape[0], labels[0].shape[1]))
      
      #prep data
      for m in range(k):
        if j != m:
          features_t = np.vstack((features_t, features[m]))
          labels_t = np.vstack((labels_t, labels[m]))
        else:
          features_v = np.vstack((features_v, features[m]))
          labels_v = np.vstack((labels_v, labels[m]))

      #SVM
      svm = SVC(kernel='rbf', gamma=g_vals[i])
      svm.fit(features_t, labels_t)
      svm_acc = svm.score(features_v, labels_v)
      accuracy_store[i, j] = svm_acc
      print(svm_acc)
  return accuracy_store

# train hard margin SVM using Inception features
start = time.time()
g_vals = np.array([0.0625, 0.125, 0.25, 0.5, 1, 2, 64])
features, labels = stratified_k_fold( dataset1["inception_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies_h = hardMarginSVM(features, labels, g_vals, 5)
stop = time.time()
print("TIME OF training hard margin SVM using INCEPTION features: ", stop-start)

start = time.time()
# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_h)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_h)%len(g_vals)]
g_vals = np.array([optimum_g])

# test Inception features on hard margin SVM 
print("***Testing Inception features on Hard Margin SVM***")
softMarginSVMTest(dataset1["inception_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["inception_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_g)
stop = time.time()
print("TIME OF testing hard margin SVM using INCEPTION features: ", stop-start)

# train hard margin SVM using HOG features
start = time.time()
g_vals = np.array([0.0625, 0.125, 0.25, 0.5, 1, 2, 64])
features, labels = stratified_k_fold( dataset1["hog_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies_h = hardMarginSVM(features, labels, g_vals, 5)

stop = time.time()
print("TIME OF training hard margin SVM using HOG features: ", stop-start)

# obtain optimal value of gamma
start = time.time()
print(g_vals[np.argmax(accuracies_h)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_h)%len(g_vals)]
g_vals = np.array([optimum_g])

# test HOG features on hard margin SVM 
print("***Testing HOG features on Hard Margin SVM***")
softMarginSVMTest(dataset1["hog_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["hog_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_g)

stop = time.time()
print("TIME OF test hard margin SVM using HOG features: ", stop-start)

"""Q 1.6"""

# soft margin SVM training algo witch rbf kernel
def softMarginSVM_rbf( features, labels, c_vals, g_vals, k): 
  accuracy_store = np.zeros((len(np.atleast_1d(c_vals))*len(np.atleast_1d(g_vals)), k))
  
  for i in range(len(np.atleast_1d(c_vals))):
      print(i)
      for g in range(len(np.atleast_1d(g_vals))):

        for j in range(k):
          features_t = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_t = np.zeros((labels[0].shape[0], labels[0].shape[1]))

          features_v = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_v = np.zeros((labels[0].shape[0], labels[0].shape[1]))
              
          #prep data
          for m in range(k):
            if j != m:
              features_t = np.vstack((features_t, features[m]))
              labels_t = np.vstack((labels_t, labels[m]))
            else:
              features_v = np.vstack((features_v, features[m]))
              labels_v = np.vstack((labels_v, labels[m]))
          #SVM  
          svm = SVC(kernel='rbf', C=c_vals[i], gamma=g_vals[g])
          svm.fit(features_t, labels_t)
          svm_acc = svm.score(features_v, labels_v)
          accuracy_store[i, j] = svm_acc
          print(svm_acc)
  return accuracy_store

# soft margin SVM test algo with rbf kernel
def softMarginSVMTest_rbf(features_t, labels_t, features_test, labels_test, c_optimal, g_optimal):
    svm = SVC(kernel='rbf', C=c_optimal, gamma=g_optimal)
    svm.fit(features_t, labels_t)
    pred = svm.predict(features_test).reshape(-1,1)
    test_SVM(pred, dataset1["superclass_labels_test"])

# train soft margin SVM using Inception features
start = time.time()
g_vals = np.array([0.25, 2, 64])
c_vals = np.array([0.01, 1, 100])
features, labels = stratified_k_fold( dataset1["inception_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies_s = softMarginSVM_rbf(features, labels, c_vals, g_vals, 5)

stop = time.time()
print("TIME OF training soft margin SVM with rbf kernel using INCEPTION features: ", stop-start)

start = time.time()
# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_s)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_s)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of C
print(c_vals[np.argmax(accuracies_s)%len(c_vals)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies_s)%len(c_vals)]
c_vals = np.array([optimum_c])

# test Inception features on soft margin SVM with rbf kernel 
print("***Testing Inception features on Soft Margin SVM***")
softMarginSVMTest_rbf(dataset1["inception_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["inception_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_c, optimum_g)

stop = time.time()
print("TIME OF testing soft margin SVM with rbf kernel using INCEPTION features: ", stop-start)

# train soft margin SVM using HOG features
start = time.time()
g_vals = np.array([0.25, 2, 64])
c_vals = np.array([0.01, 0.1, 100])
features, labels = stratified_k_fold( dataset1["hog_features_train"],
                                     dataset1["superclass_labels_train"], 5)
accuracies_s = softMarginSVM_rbf(features, labels, c_vals, g_vals, 5)

stop = time.time()
print("TIME OF training soft margin SVM with rbf kernel using HOG features: ", stop-start)

# obtain optimal value of gamma
start = time.time()
print(g_vals[np.argmax(accuracies_s)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_s)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of C
print(c_vals[np.argmax(accuracies_s)%len(c_vals)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies_s)%len(c_vals)]
c_vals = np.array([optimum_c])

# test HOG features on soft margin SVM with rbf kernel 
print("***Testing HOG features on Soft Margin SVM***")
softMarginSVMTest_rbf(dataset1["hog_features_train"],
                  dataset1["superclass_labels_train"],
                  dataset1["hog_features_test"],
                  dataset1["superclass_labels_test"],
                  optimum_c, optimum_g)

stop = time.time()
print("TIME OF testing soft margin SVM with rbf kernel using HOG features: ", stop-start)

"""Q 1.7"""

# soft margin SVM training algo witch rbf kernel
from sklearn.multiclass import OneVsRestClassifier
def softMarginSVM_subclass( features, labels, c_vals, g_vals, k): 
  accuracy_store = np.zeros((len(np.atleast_1d(c_vals)),len(np.atleast_1d(g_vals)), k))
  
  for i in range(len(np.atleast_1d(c_vals))):
      print(i)
      for g in range(len(np.atleast_1d(g_vals))):
        for j in range(k):
          features_t = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_t = np.zeros((labels[0].shape[0], labels[0].shape[1]))

          features_v = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_v = np.zeros((labels[0].shape[0], labels[0].shape[1]))
              
          # prep data
          for m in range(k):
            if j != m:
              features_t = np.vstack((features_t, features[m]))
              labels_t = np.vstack((labels_t, labels[m]))
            else:
              features_v = np.vstack((features_v, features[m]))
              labels_v = np.vstack((labels_v, labels[m]))

          # SVM
          classifier = OneVsRestClassifier(SVC(kernel='rbf', C=c_vals[i], gamma=g_vals[g]))
          classifier.fit(features_t, labels_t)
          svm_acc = classifier.score(features_v, labels_v)
          accuracy_store[i, g, j] = svm_acc
  return accuracy_store

# soft margin SVM test algo with rbf kernel
def softMarginSVMTest_ovr(features_t, labels_t, features_test, labels_test, c_optimal, g_optimal,flag=0):
    classifier = OneVsRestClassifier(SVC(kernel='rbf', C=c_optimal, gamma=g_optimal))
    classifier.fit(features_t, labels_t)

    pred = classifier.predict(features_test).reshape(-1,1)
    test_SVM(pred,dataset1["subclass_labels_test"],flag)

# train soft margin SVM using Inception features
start = time.time()
g_vals = np.array([0.25, 2, 64])
c_vals = np.array([0.01, 1, 100])
features, labels = stratified_k_fold( dataset1["inception_features_train"],
                                     dataset1["subclass_labels_train"], 5)
accuracies_ovr = softMarginSVM_subclass(features, labels, c_vals, g_vals, 5)
stop = time.time()
print("TIME OF training soft margin SVM with One vs All rbf kernel using Inception features: ", stop-start)

start = time.time()
# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_ovr)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_ovr)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of C
print(c_vals[np.argmax(accuracies_ovr)%len(c_vals)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies_ovr)%len(c_vals)]
c_vals = np.array([optimum_c])

# test Inception features on soft margin SVM with rbf kernel 
print("***Testing Inception features on Soft Margin SVM with One vs All***")
softMarginSVMTest_ovr(dataset1["inception_features_train"],
                  dataset1["subclass_labels_train"],
                  dataset1["inception_features_test"],
                  dataset1["subclass_labels_test"],
                  optimum_c, optimum_g,1)
stop = time.time()
print("TIME OF testing soft margin SVM with One vs All using Inception features: ", stop-start)

# train soft margin SVM using HOG features
start = time.time()
g_vals = np.array([0.25, 2, 64])
c_vals = np.array([0.01, 1, 100])
features, labels = stratified_k_fold( dataset1["hog_features_train"],
                                     dataset1["subclass_labels_train"], 5)
accuracies_ovr = softMarginSVM_subclass(features, labels, c_vals, g_vals, 5)

stop = time.time()
print("TIME OF training soft margin SVM with One vs All using HOG features: ", stop-start)

start = time.time()
# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_ovr)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_ovr)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of C
print(c_vals[np.argmax(accuracies_ovr)%len(c_vals)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies_ovr)%len(c_vals)]
c_vals = np.array([optimum_c])

# test HOG features on soft margin SVM with rbf kernel 
print("***Testing HOG features on Soft Margin SVM with One vs All***")
softMarginSVMTest_ovr(dataset1["hog_features_train"],
                  dataset1["subclass_labels_train"],
                  dataset1["hog_features_test"],
                  dataset1["subclass_labels_test"],
                  optimum_c, optimum_g,1)

stop = time.time()
print("TIME OF testing soft margin SVM with One vs All using HOG features: ", stop-start)

"""Q 1.8"""

# hard margin SVM training algo
def hardMarginSVM_subclass( features, labels, g_vals, d_vals, k): 
  accuracy_store = np.zeros((len(np.atleast_1d(d_vals)),len(np.atleast_1d(g_vals)), k))

  for g in range(len(np.atleast_1d(d_vals))):
      print(g)
      for i in range(len(np.atleast_1d(g_vals))):
        for j in range(k):
          features_t = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_t = np.zeros((labels[0].shape[0], labels[0].shape[1]))

          features_v = np.zeros((features[0].shape[0], features[0].shape[1]))
          labels_v = np.zeros((labels[0].shape[0], labels[0].shape[1]))
          
          #prep data
          for m in range(k):
            if j != m:
              features_t = np.vstack((features_t, features[m]))
              labels_t = np.vstack((labels_t, labels[m]))
            else:
              features_v = np.vstack((features_v, features[m]))
              labels_v = np.vstack((labels_v, labels[m]))
          
          # SVM
          classifier = OneVsRestClassifier(SVC(kernel='poly', degree=d_vals[g], gamma=g_vals[i]))
          classifier.fit(features_t, labels_t)

          svm_acc = classifier.score(features_v, labels_v)
          accuracy_store[g, i, j] = svm_acc
  return accuracy_store

# hard margin SVM test algo with rbf kernel
def hardMarginSVMTest_ovr(features_t, labels_t, features_test, labels_test, g_optimal, d_optimal,flag=0):
    classifier = OneVsRestClassifier(SVC(kernel='poly', degree=d_optimal, gamma=g_optimal))
    classifier.fit(features_t, labels_t)

    pred = classifier.predict(features_test).reshape(-1,1)
    test_SVM(pred,dataset1["subclass_labels_test"],flag)

# train hard margin SVM using Inception features
start = time.time()
g_vals = np.array([0.25, 2, 64])
d_vals = np.array([3, 5, 7])
features, labels = stratified_k_fold( dataset1["inception_features_train"],
                                     dataset1["subclass_labels_train"], 5)
accuracies_ovr = hardMarginSVM_subclass(features, labels, g_vals, d_vals, 5)
stop = time.time()
print("TIME OF training hard margin SVM with One vs All rbf kernel using Inception features: ", stop-start)

start = time.time()

# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_ovr)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_ovr)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of D
print(d_vals[np.argmax(accuracies_ovr)%len(d_vals)], " is the optimal value of d")
optimum_d = d_vals[np.argmax(accuracies_ovr)%len(d_vals)]
d_vals = np.array([optimum_d])

# test Inception features on soft margin SVM with rbf kernel 
print("***Testing Inception features on Hard Margin SVM with One vs All***")
hardMarginSVMTest_ovr(dataset1["inception_features_train"],
                  dataset1["subclass_labels_train"],
                  dataset1["inception_features_test"],
                  dataset1["subclass_labels_test"],
                  optimum_g, optimum_d,1)
stop = time.time()
print("TIME OF testing hard margin SVM with One vs All using Inception features: ", stop-start)

# train hard margin SVM using HOG features
start = time.time()
g_vals = np.array([0.25, 2, 64])
c_vals = np.array([0.01, 1, 100])
features, labels = stratified_k_fold( dataset1["hog_features_train"],
                                     dataset1["subclass_labels_train"], 5)
accuracies_ovr = softMarginSVM_subclass(features, labels, c_vals, g_vals, 5)
stop = time.time()
print("TIME OF training hard margin SVM with One vs All rbf kernel using HOG features: ", stop-start)

start = time.time()
# obtain optimal value of gamma
print(g_vals[np.argmax(accuracies_ovr)%len(g_vals)], " is the optimal value of gamma")
optimum_g = g_vals[np.argmax(accuracies_ovr)%len(g_vals)]
g_vals = np.array([optimum_g])

# obtain optimal value of C
print(c_vals[np.argmax(accuracies_ovr)%len(c_vals)], " is the optimal value of C")
optimum_c = c_vals[np.argmax(accuracies_ovr)%len(c_vals)]
c_vals = np.array([optimum_c])

# test Inception features on soft margin SVM with rbf kernel 
print("***Testing HOG features on Hard Margin SVM with One vs All***")
softMarginSVMTest_ovr(dataset1["hog_features_train"],
                  dataset1["subclass_labels_train"],
                  dataset1["hog_features_test"],
                  dataset1["subclass_labels_test"],
                  optimum_c, optimum_g,1)
stop = time.time()
print("TIME OF testing hard margin SVM with One vs All using HOG features: ", stop-start)

"""Q 2.1"""

# loading dataset and reshape
dataset2 = scipy.io.loadmat('q2_dataset.mat')
dataset2["data"] = np.reshape(dataset2["data"], (150, 10625))
dataset2 = dataset2["data"]

"""Q 2.2"""

# finding the covariance matrix
feature_vectors = dataset2.T
cov_matrix = np.cov(feature_vectors)
eigen_values, eigen_vectors = np.linalg.eig(cov_matrix) 
np.sort(eigen_values)
np.sort(eigen_vectors)

# using the top 3
eigen_vals_first3  = eigen_values[:3]
eigen_vecs_first3 = eigen_vectors[:,:3]

# variance 
covered_var = eigen_values[:3].sum() / eigen_values.sum()
print("% of variance covered is: ",covered_var)

# projection
projected_fv = dataset2.dot(eigen_vectors.T[:,:3])
